{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder for Regression - WIP\n",
    "\n",
    "- Paper https://arxiv.org/abs/1904.05948\n",
    "\n",
    "- Repository https://github.com/QingyuZhao/VAE-for-Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error as mse, r2_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Toy Example Data\n",
    "training_feature = np.loadtxt('data/X.txt')\n",
    "training_feature.shape\n",
    "\n",
    "Y = np.loadtxt('data/Y.txt')\n",
    "ground_truth_r = Y\n",
    "\n",
    "np.random.seed(seed=0)\n",
    "\n",
    "original_dim = training_feature.shape[1]\n",
    "num_train = training_feature.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(mean, log_var):\n",
    "    '''\n",
    "    Arguments:\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    Returns:\n",
    "        z (tensor): sampled latent vector\n",
    "    '''\n",
    "    print(type(mean), type(log_var))\n",
    "    epsilon = torch.randn_like(torch.Tensor(mean)) \n",
    "    return mean + torch.exp(0.5*log_var)*epsilon "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build VAE Regression Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_shape_x, intermediate_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "        self.fc1 = nn.Linear(input_shape_x, 128)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(128, intermediate_dim)\n",
    "        self.act2 = nn.Tanh() \n",
    "        \n",
    "        # posterior on Y; probabilistic regressor \n",
    "        self.r_mean_layer = nn.Linear(intermediate_dim, 1)\n",
    "        self.r_logvar_layer = nn.Linear(intermediate_dim, 1) \n",
    "\n",
    "        # q(z|x) \n",
    "        self.z_mean_layer = nn.Linear(intermediate_dim, latent_dim)\n",
    "        self.z_logvar_layer = nn.Linear(intermediate_dim, latent_dim)\n",
    "\n",
    "        # latent generator \n",
    "        self.gen_z = weight_norm(nn.Linear(1, latent_dim))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        x = self.act1(self.fc1(x))\n",
    "        x = self.act2(self.fc2(x))\n",
    "\n",
    "        r_mean = self.r_mean_layer(x)\n",
    "        r_logvar = self.r_logvar_layer(x)\n",
    "\n",
    "        z_mean = self.z_mean_layer(x)\n",
    "        z_logvar = self.z_logvar_layer(x)\n",
    "        \n",
    "        \n",
    "        # reparameterization trick\n",
    "        r = sampling(r_mean, r_logvar)\n",
    "        z = sampling(z_mean, z_logvar)\n",
    "\n",
    "        pz_mean = self.gen_z(r) \n",
    "\n",
    "        return r_mean, r_logvar, r, z_mean, z_logvar, z, pz_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_shape_x, intermediate_dim, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, intermediate_dim)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(intermediate_dim, 128)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.fc3 = nn.Linear(128, input_shape_x)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.fc1(x))\n",
    "        x = self.act2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(original_dim, 32, 8) #to(device)\n",
    "decoder = Decoder(original_dim, 32, 8) #.to(device) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = torch.Tensor(images)\n",
    "        self.labels = torch.Tensor(labels) \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        imgs = self.images[idx]\n",
    "        lbls = self.labels[idx]\n",
    "        return imgs, lbls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "intermidiate_dim = 32\n",
    "batch_size = 64\n",
    "latent_dim = 8\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruction_loss = mse(inputs_x,outputs)\n",
    "# #kl_loss = 1 + z_log_var - pz_log_var - K.tf.divide(K.square(z_mean-pz_mean),K.exp(pz_log_var)) - K.tf.divide(K.exp(z_log_var),K.exp(pz_log_var))\n",
    "# kl_loss = 1 + z_log_var - K.square(z_mean-pz_mean) - K.exp(z_log_var)\n",
    "# kl_loss = -0.5*K.sum(kl_loss, axis=-1)\n",
    "# label_loss = K.tf.divide(0.5*K.square(r_mean - inputs_r), K.exp(r_log_var)) +  0.5 * r_log_var\n",
    "\n",
    "# vae_loss = K.mean(reconstruction_loss+kl_loss+label_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(inputs_x, outputs, inputs_r, z_mean, z_log_var, r_mean, r_logvar, pz_mean):\n",
    "    reconstruction_loss = mse(inputs_x,outputs)\n",
    "    kl_loss = 1 + z_logvar - torch.square(z_mean-pz_mean) - torch.exp(z_logvar)\n",
    "    kl_loss = -0.5 * torch.sum(kl_loss) \n",
    "    \n",
    "    print(r_mean.shape, inputs_r.shape, type(r_mean), type(inputs_r))\n",
    "    \n",
    "    label_loss = torch.divide(0.5 * torch.square(r_mean - inputs_r), troch.exp(r_logvar)) +  0.5 * r_logvar\n",
    "    return torch.mean(reconstruction_loss+kl_loss+label_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # KL divergence\n",
    "# def loss_fn(out, imgs, mu, logVar):\n",
    "#     kl_divergence = 0.5 * torch.sum(1 + logVar - mu.pow(2) - logVar.exp())\n",
    "#     return F.binary_cross_entropy(out, imgs, size_average=False) - kl_divergence\n",
    "\n",
    "# lr= 0.001 # Learning rate\n",
    "\n",
    "# params_to_optimize = [\n",
    "#     {'params': encoder.parameters()},\n",
    "#     {'params': decoder.parameters()}\n",
    "# ]\n",
    "\n",
    "# optim = torch.optim.Adam(params_to_optimize, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optmizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ground_truth_r)\n",
    "#training_feature_sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "torch.Size([64, 8]) torch.Size([64]) <class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (64) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 38\u001b[0m\n\u001b[0;32m     32\u001b[0m r_mean, r_logvar, r, z_mean, z_logvar, z, pz_mean \u001b[38;5;241m=\u001b[39m encoder(imgs) \n\u001b[0;32m     34\u001b[0m recon_imgs \u001b[38;5;241m=\u001b[39m decoder(z)\n\u001b[1;32m---> 38\u001b[0m vae_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecon_imgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_logvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_logvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpz_mean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m#print(type(r_mean), type(r_mean), type(r_mean), type(r_mean), type(r_mean))\u001b[39;00m\n\u001b[0;32m     43\u001b[0m optmizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Cell \u001b[1;32mIn[12], line 8\u001b[0m, in \u001b[0;36mloss_function\u001b[1;34m(inputs_x, outputs, inputs_r, z_mean, z_log_var, r_mean, r_logvar, pz_mean)\u001b[0m\n\u001b[0;32m      4\u001b[0m kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(kl_loss) \n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(r_mean\u001b[38;5;241m.\u001b[39mshape, inputs_r\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;28mtype\u001b[39m(r_mean), \u001b[38;5;28mtype\u001b[39m(inputs_r))\n\u001b[1;32m----> 8\u001b[0m label_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdivide(\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msquare(\u001b[43mr_mean\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs_r\u001b[49m), troch\u001b[38;5;241m.\u001b[39mexp(r_logvar)) \u001b[38;5;241m+\u001b[39m  \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m r_logvar\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmean(reconstruction_loss\u001b[38;5;241m+\u001b[39mkl_loss\u001b[38;5;241m+\u001b[39mlabel_loss)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (64) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "## Train the network\n",
    "np.random.seed(0)\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "pred = np.zeros((ground_truth_r.shape))\n",
    "fake = np.zeros((ground_truth_r.shape[0]))\n",
    "fake[:300] = 1\n",
    "\n",
    "\n",
    "# Run 10-fold CV\n",
    "for train_idx, test_idx in skf.split(training_feature, fake):\n",
    "    \n",
    "    training_feature_sk = training_feature[train_idx,:]\n",
    "    training_score = ground_truth_r[train_idx]\n",
    "    testing_feature_sk = training_feature[test_idx,:]\n",
    "    testing_score = ground_truth_r[test_idx]    \n",
    "    \n",
    "    train = VAEDataset(training_feature_sk, training_score)\n",
    "    test = VAEDataset(testing_feature_sk, testing_score)\n",
    "    train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=True) \n",
    "    test_dataloader = DataLoader(test, batch_size=batch_size, shuffle=False)     \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "#         encoder.train()\n",
    "#         decoder.train() \n",
    "        \n",
    "        for idx, data in enumerate(train_dataloader):\n",
    "        \n",
    "            imgs, labels = data\n",
    "            #imgs = imgs #.to(device)\n",
    "            \n",
    "            r_mean, r_logvar, r, z_mean, z_logvar, z, pz_mean = encoder(imgs) \n",
    "            \n",
    "            recon_imgs = decoder(z)\n",
    "            \n",
    "            \n",
    "            \n",
    "            vae_loss = loss_function(imgs, recon_imgs, labels, r_mean, r_logvar, z_mean, z_logvar, pz_mean)\n",
    "            \n",
    "            #print(type(r_mean), type(r_mean), type(r_mean), type(r_mean), type(r_mean))\n",
    "            \n",
    "            \n",
    "            optmizer.zero_grad()\n",
    "            \n",
    "            vae_loss.backward()\n",
    "            \n",
    "            optmizer.step()\n",
    "            \n",
    "#         # test phase\n",
    "#         encoder.eval()\n",
    "#         decoder.eval()\n",
    "        with torch.no_grad():\n",
    "            x = Tensor(testing_feature_sk) #.cuda()\n",
    "            r_mean, r_logvar, r, z_mean, z_logvar, z, pz_mean = encoder(x)\n",
    "            #[mu_z, logvar_z, z], [mu_y, logvar_y, y], z_bar_y = encoder(x)\n",
    "            \n",
    "            pred[test_idx] = np.array(mu_y.cpu().detach())[:,0]\n",
    "            rmse_loss = rmse(mu_y, Tensor(testing_score).cuda())\n",
    "        # print\n",
    "        print(\"[Epoch: %d/%d] [Train loss: %.3f] ---> [Test RMSE: %.3f]\" \\\n",
    "              % (epoch + 1, epochs, total_vae_loss/(i + 1), rmse_loss.item()))    \n",
    "    print(test_idx)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mean squared error\n",
    "# print(\"Mean squared error: %.3f\" % mean_squared_error(ground_truth_r, pred))\n",
    "# # Explained variance score: 1 is perfect prediction\n",
    "# print('R2 Variance score: %.3f' % r2_score(ground_truth_r, pred))\n",
    "\n",
    "# # Plot Prediction vs. Ground-truth Y\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "\n",
    "# ax.scatter(ground_truth_r, pred,  color='black')\n",
    "# plt.xlabel('ground truth')\n",
    "# plt.ylabel('prediction truth')\n",
    "# ax.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Latent Space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.load_weights('random_weights.h5')\n",
    "vae.fit([training_feature,ground_truth_r],\n",
    "         epochs=epochs,\n",
    "         batch_size=batch_size,\n",
    "         verbose = 0)\n",
    " \n",
    "[z_mean, z_log_var, z, r_mean, r_log_var, r_vae, pz_mean] = encoder.predict([training_feature,ground_truth_r],batch_size=batch_size)\n",
    "\n",
    "tsne = MDS(n_components=2, random_state=0)\n",
    "X_2d = tsne.fit_transform(z_mean)\n",
    "\n",
    "#%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(X_2d[:, 0], X_2d[:, 1], c=ground_truth_r)\n",
    "plt.title('TSNE visualization of latent space')\n",
    "ax.axis('equal')\n",
    "\n",
    "## Train the network\n",
    "np.random.seed(0)\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "pred = np.zeros((ground_truth_r.shape))\n",
    "fake = np.zeros((ground_truth_r.shape[0]))\n",
    "fake[:300] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{aligned}\n",
    "& K L\\left(N\\left(\\mu, \\sigma^{2}\\right) \\| N(0,1)\\right) \\\\\n",
    "=& \\int \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^\\frac{-(x-\\mu)^{2}}{2 \\sigma^{2}}\\left(\\log \\frac{e^{-(x-\\mu)^{2} / 2 \\sigma^{2}} / \\sqrt{2 \\pi \\sigma^{2}}}{e^{-x^{2} / 2} / \\sqrt{2 \\pi}}\\right) d x\\\\\n",
    "=& \\int \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^\\frac{-(x-\\mu)^{2}}{2 \\sigma^{2}} \\log \\left\\{\\frac{1}{\\sqrt{\\sigma^{2}}} \\exp \\left\\{\\frac{1}{2}\\left[x^{2}-(x-\\mu)^{2} / \\sigma^{2}\\right]\\right\\}\\right\\} d x \\\\\n",
    "=& \\frac{1}{2} \\int \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^\\frac{-(x-\\mu)^{2}}{2 \\sigma^{2}} \\left[ -\\log \\sigma^{2}+x^{2}-(x-\\mu)^{2} / \\sigma^{2}\\right] d x\n",
    "\\end{aligned} $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
