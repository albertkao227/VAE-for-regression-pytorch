{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder for Regression - WIP\n",
    "\n",
    "- Paper https://arxiv.org/abs/1904.05948\n",
    "\n",
    "- Repository https://github.com/QingyuZhao/VAE-for-Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error as mse, r2_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Toy Example Data\n",
    "training_feature = np.loadtxt('data/X.txt')\n",
    "training_feature.shape\n",
    "\n",
    "Y = np.loadtxt('data/Y.txt')\n",
    "ground_truth_r = Y\n",
    "\n",
    "np.random.seed(seed=0)\n",
    "\n",
    "original_dim = training_feature.shape[1]\n",
    "num_train = training_feature.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    '''\n",
    "    Arguments:\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    Returns:\n",
    "        z (tensor): sampled latent vector\n",
    "    '''\n",
    "    mean, log_var = args\n",
    "    epsilon = torch.randn_like(torch.Tensor(mean)) \n",
    "    return mean + torch.exp(0.5*log_var)*epsilon "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build VAE Regression Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_shape_x, intermediate_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "        self.fc1 = nn.Linear(input_shape_x, 128)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(128, intermediate_dim)\n",
    "        self.act2 = nn.Tanh() \n",
    "        \n",
    "        # posterior on Y; probabilistic regressor \n",
    "        self.r_mean = nn.Linear(intermediate_dim, 1)\n",
    "        self.r_logvar = nn.Linear(intermediate_dim, 1) \n",
    "\n",
    "        # q(z|x) \n",
    "        self.z_mean = nn.Linear(intermediate_dim, latent_dim)\n",
    "        self.z_logvar = nn.Linear(intermediate_dim, latent_dim)\n",
    "\n",
    "        # latent generator \n",
    "        self.gen_z = weight_norm(nn.Linear(1, latent_dim))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        x = self.act1(self.fc1(x))\n",
    "        x = self.act2(self.fc2(x))\n",
    "\n",
    "        r_mean = self.r_mean(x)\n",
    "        r_logvar = self.r_logvar(x)\n",
    "\n",
    "        z_mean = self.z_mean(x)\n",
    "        z_logvar = self.z_logvar(x)\n",
    "\n",
    "        # reparameterization trick\n",
    "        r = sampling([self.r_mean, self.r_logvar])\n",
    "        z = sampling([self.z_mean, self.z_logvar])\n",
    "\n",
    "        pz_mean = self.gen_z(r) \n",
    "\n",
    "        return r_mean, r_logvar, r, z_mean, z_logvar, z, pz_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_shape_x, intermediate_dim, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, intermediate_dim)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(intermediate_dim, 128)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.fc3 = nn.Linear(128, input_shape_x)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.fc1(x))\n",
    "        x = self.act2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(original_dim, 32, 8).to(device)\n",
    "decoder = Decoder(original_dim, 32, 8).to(device) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = torch.Tensor(images)\n",
    "        self.labels = torch.Tensor(labels) \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        imgs = self.images[idx]\n",
    "        lbls = self.labels[idx]\n",
    "        return imgs, lbls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "intermidiate_dim = 32\n",
    "batch_size = 64\n",
    "latent_dim = 8\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruction_loss = mse(inputs_x,outputs)\n",
    "# #kl_loss = 1 + z_log_var - pz_log_var - K.tf.divide(K.square(z_mean-pz_mean),K.exp(pz_log_var)) - K.tf.divide(K.exp(z_log_var),K.exp(pz_log_var))\n",
    "# kl_loss = 1 + z_log_var - K.square(z_mean-pz_mean) - K.exp(z_log_var)\n",
    "# kl_loss = -0.5*K.sum(kl_loss, axis=-1)\n",
    "# label_loss = K.tf.divide(0.5*K.square(r_mean - inputs_r), K.exp(r_log_var)) +  0.5 * r_log_var\n",
    "\n",
    "# vae_loss = K.mean(reconstruction_loss+kl_loss+label_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(inputs_x, outputs, inputs_r, z_mean, z_log_var, r_mean, r_logvar, pz_mean):\n",
    "    reconstruction_loss = mse(inputs_x,outputs)\n",
    "    kl_loss = 1 + z_logvar - torch.square(z_mean-pz_mean) - torch.exp(z_logvar)\n",
    "    kl_loss = -0.5 * torch.sum(kl_loss) \n",
    "    label_loss = torch.divide(0.5 * torch.square(r_mean - inputs_r), troch.exp(r_logvar)) +  0.5 * r_logvar\n",
    "    return torch.mean(reconstruction_loss+kl_loss+label_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # KL divergence\n",
    "# def loss_fn(out, imgs, mu, logVar):\n",
    "#     kl_divergence = 0.5 * torch.sum(1 + logVar - mu.pow(2) - logVar.exp())\n",
    "#     return F.binary_cross_entropy(out, imgs, size_average=False) - kl_divergence\n",
    "\n",
    "lr= 0.001 # Learning rate\n",
    "\n",
    "params_to_optimize = [\n",
    "    {'params': encoder.parameters()},\n",
    "    {'params': decoder.parameters()}\n",
    "]\n",
    "\n",
    "optim = torch.optim.Adam(params_to_optimize, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optmizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ground_truth_r\n",
    "#training_feature_sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Train the network\n",
    "# np.random.seed(0)\n",
    "# skf = StratifiedKFold(n_splits=10)\n",
    "# pred = np.zeros((ground_truth_r.shape))\n",
    "# fake = np.zeros((ground_truth_r.shape[0]))\n",
    "# fake[:300] = 1\n",
    "\n",
    "\n",
    "# # Run 10-fold CV\n",
    "# for train_idx, test_idx in skf.split(training_feature, fake):\n",
    "#     training_feature_sk = training_feature[train_idx,:]\n",
    "#     training_score = ground_truth_r[train_idx]\n",
    "#     testing_feature_sk = training_feature[test_idx,:]\n",
    "#     testing_score = ground_truth_r[test_idx]    \n",
    "    \n",
    "#     train = VAEDataset(training_feature_sk, training_score)\n",
    "#     test = VAEDataset(testing_feature_sk, testing_score)\n",
    "#     train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=True) \n",
    "#     test_dataloader = DataLoader(test, batch_size=batch_size, shuffle=False)     \n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "        \n",
    "#         encoder.train()\n",
    "#         decoder.train() \n",
    "        \n",
    "#         for idx, data in enumerate(train_dataloader):\n",
    "        \n",
    "#             imgs, lbls = data\n",
    "#             imgs = imgs.to(device)\n",
    "#             lbls = lbls.to(device)\n",
    "            \n",
    "#             r_mean, r_logvar, r, z_mean, z_logvar, z, pz_mean = encoder(imgs)\n",
    "            \n",
    "#             vae_loss = loss_function(inputs_x, outputs, lbls, r_mean, r_logvar, z_mean, z_log_var, pz_mean)\n",
    "            \n",
    "            \n",
    "#             optmizer.zero_grad()\n",
    "            \n",
    "#             vae_loss.backward()\n",
    "            \n",
    "#             optmizer.step()\n",
    "            \n",
    "            \n",
    "#     print(test_idx)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mean squared error\n",
    "# print(\"Mean squared error: %.3f\" % mean_squared_error(ground_truth_r, pred))\n",
    "# # Explained variance score: 1 is perfect prediction\n",
    "# print('R2 Variance score: %.3f' % r2_score(ground_truth_r, pred))\n",
    "\n",
    "# # Plot Prediction vs. Ground-truth Y\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "\n",
    "# ax.scatter(ground_truth_r, pred,  color='black')\n",
    "# plt.xlabel('ground truth')\n",
    "# plt.ylabel('prediction truth')\n",
    "# ax.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Latent Space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.load_weights('random_weights.h5')\n",
    "vae.fit([training_feature,ground_truth_r],\n",
    "         epochs=epochs,\n",
    "         batch_size=batch_size,\n",
    "         verbose = 0)\n",
    " \n",
    "[z_mean, z_log_var, z, r_mean, r_log_var, r_vae, pz_mean] = encoder.predict([training_feature,ground_truth_r],batch_size=batch_size)\n",
    "\n",
    "tsne = MDS(n_components=2, random_state=0)\n",
    "X_2d = tsne.fit_transform(z_mean)\n",
    "\n",
    "#%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(X_2d[:, 0], X_2d[:, 1], c=ground_truth_r)\n",
    "plt.title('TSNE visualization of latent space')\n",
    "ax.axis('equal')\n",
    "\n",
    "## Train the network\n",
    "np.random.seed(0)\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "pred = np.zeros((ground_truth_r.shape))\n",
    "fake = np.zeros((ground_truth_r.shape[0]))\n",
    "fake[:300] = 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
