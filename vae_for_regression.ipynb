{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder for Regression \n",
    "\n",
    "- Paper https://arxiv.org/abs/1904.05948\n",
    "\n",
    "- Repository https://github.com/QingyuZhao/VAE-for-Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Toy Example Data\n",
    "training_feature = np.loadtxt('data/X.txt')\n",
    "training_feature.shape\n",
    "\n",
    "Y = np.loadtxt('data/Y.txt')\n",
    "ground_truth_r = Y\n",
    "\n",
    "np.random.seed(seed=0)\n",
    "\n",
    "original_dim = training_feature.shape[1]\n",
    "num_train = training_feature.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    '''\n",
    "    Arguments:\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    Returns:\n",
    "        z (tensor): sampled latent vector\n",
    "    '''\n",
    "    mean, log_var = args\n",
    "    epsilon = torch.randn_like(mean) \n",
    "    return mean + torch.exp(0.5*log_var)*epsilon "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build VAE Regression Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_shape_x, intermediate_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "        self.fc1 = nn.Linear(input_shape_x, 128)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(128, intermediate_dim)\n",
    "        self.act2 = nn.Tanh() \n",
    "        \n",
    "        # posterior on Y; probabilistic regressor \n",
    "        self.r_mean = nn.Linear(intermediate_dim, 1)\n",
    "        self.r_logvar = nn.Linear(intermediate_dim, 1) \n",
    "\n",
    "        # q(z|x) \n",
    "        self.z_mean = nn.Linear(intermediate_dim, latent_dim)\n",
    "        self.z_logvar = nn.Linear(intermediate_dim, latent_dim)\n",
    "\n",
    "        # latent generator \n",
    "        self.gen_z = weight_norm(nn.Linear(1, latent_dim))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        x = self.act1(self.fc1(x))\n",
    "        x = self.act2(self.fc2(x))\n",
    "\n",
    "        r_mean = self.r_mean(x)\n",
    "        r_logvar = self.r_logvar(x)\n",
    "\n",
    "        z_mean = self.z_mean(x)\n",
    "        z_logvar = self.z_logvar(x)\n",
    "\n",
    "        # reparameterization trick\n",
    "        r = sampling(self.r_mean, self.r_logvar)\n",
    "        z = sampling(self.z_mean, self.z_logvar)\n",
    "\n",
    "        pz_mean = self.gen_z(r) \n",
    "\n",
    "        return r_mean, r_logvar, r, z_mean, z_logvar, z, pz_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_shape_x, intermediate_dim, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, intermediate_dim)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(intermediate_dim, 128)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.fc3 = nn.Linear(128, input_shape_x)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.fc1(x))\n",
    "        x = self.act2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(original_dim, 32, 8)\n",
    "decoder = Decoder(original_dim, 32, 8) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermidiate_dim = 32\n",
    "batch_size = 64\n",
    "latent_dim = 8\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optmizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the network\n",
    "np.random.seed(0)\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "pred = np.zeros((ground_truth_r.shape))\n",
    "fake = np.zeros((ground_truth_r.shape[0]))\n",
    "fake[:300] = 1\n",
    "\n",
    "\n",
    "# Run 10-fold CV\n",
    "for train_idx, test_idx in skf.split(training_feature, fake):\n",
    "    training_feature_sk = training_feature[train_idx,:]\n",
    "    training_score = ground_truth_r[train_idx]\n",
    "    testing_feature_sk = training_feature[test_idx,:]\n",
    "    testing_score = ground_truth_r[test_idx]    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mean squared error\n",
    "# print(\"Mean squared error: %.3f\" % mean_squared_error(ground_truth_r, pred))\n",
    "# # Explained variance score: 1 is perfect prediction\n",
    "# print('R2 Variance score: %.3f' % r2_score(ground_truth_r, pred))\n",
    "\n",
    "# # Plot Prediction vs. Ground-truth Y\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "\n",
    "# ax.scatter(ground_truth_r, pred,  color='black')\n",
    "# plt.xlabel('ground truth')\n",
    "# plt.ylabel('prediction truth')\n",
    "# ax.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Latent Space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.load_weights('random_weights.h5')\n",
    "vae.fit([training_feature,ground_truth_r],\n",
    "         epochs=epochs,\n",
    "         batch_size=batch_size,\n",
    "         verbose = 0)\n",
    " \n",
    "[z_mean, z_log_var, z, r_mean, r_log_var, r_vae, pz_mean] = encoder.predict([training_feature,ground_truth_r],batch_size=batch_size)\n",
    "\n",
    "tsne = MDS(n_components=2, random_state=0)\n",
    "X_2d = tsne.fit_transform(z_mean)\n",
    "\n",
    "#%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(X_2d[:, 0], X_2d[:, 1], c=ground_truth_r)\n",
    "plt.title('TSNE visualization of latent space')\n",
    "ax.axis('equal')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
